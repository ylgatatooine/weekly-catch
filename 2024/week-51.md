## Weekly Catch - Dec 22nd, 2024

The holidays are almost here, and I've got some extra weekend time to dive into some intentional reading (not quite work, not quite pleasure, but super enjoyable!). Here are my top 3 catches:

1. [Grab-a-floaty-in-the-AI-Rapids](#grab-a-floaty-in-the-ai-rapids)

Among the top arXiv papers on Huggingface, I cross-referenced them on Semantic Scholar. Here are my favorites.

2. [One Step Closer to AGI](#grab-a-floaty-in-the-ai-rapids)

OpenAI’s o3 model made a big jump with a 75.7% score on ARC-AGI. AGI is getting closer! 2nd stop of 5.

3. [AI Business in 2025](#grab-a-floaty-in-the-ai-rapids)

Sequoia Capital shares their predictions for AI business opportunities in 2025

---

## Grab a floaty in the AI Rapids

The AI race in 2024 is as intense as this year’s Summer Olympics. It’s real for me—I use it during the week and tinker with it occasionally during weekends. I mostly felt unable to catch up. Now, it is the time to  I dove into [Huggingface’s daily top-upvoted arXiv papers](https://huggingface.co/Papers) and cross-checked citations on Semantic Scholar. Here are my top picks of the year!

1. [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)  
Can BitNet’s 1-bit LLM approach revolutionize AI scaling by reducing GPU demand, energy consumption, and memory usage while maintaining high performance?

2. [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)  
Can Llama 3, with its 405B parameters and 128K token context window, outperform GPT-4 across tasks while integrating image, video, and speech capabilities, all with safety measures through Llama Guard 3?

3. [Mixtral of Experts](https://arxiv.org/abs/2401.04088)  
 Can Mixtral 8x7B, with its Sparse Mixture of Experts approach, using only 13B active parameters, outperform models like Llama 2 70B and GPT-3.5 in key tasks and set new standards for AI benchmarks?

4. [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)  
We're talking trillions now. Can Qwen2.5’s training on 18 trillion tokens provide enhanced reasoning, long-text generation, and data analysis while competing with top models like GPT-4o-mini at a fraction of the cost?

5. [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)  
Can Phi-3-mini, with its 3.8B parameters, challenge the norm by delivering GPT-3.5-level performance on mobile devices?


---

## We are 2nd step of 5 toward AGI

The release of OpenAI's o3 model marks a surprising and significant leap in AI capabilities, with a 75.7% performance on ARC-AGI, a level of task adaptation not seen before in GPT-family models. For context, ARC-AGI-1 took 4 years to progress from 0% with GPT-3 in 2020 to 5% in 2024 with GPT-4o. With o3, AI development is accelerating. [full article](https://arcprize.org/blog/oai-o3-pub-breakthrough).

OpenAI has announced that o3 reached **Level 2** of AGI. I’m reviewing OpenAI's categorization from July, which outlines the five steps to achieve AGI by 2030. 

1. **Conversational AI (Level 1)**: AI systems like ChatGPT engage in basic conversations and assist with tasks like content creation and customer support.
2. **Reasoning AI (Level 2)**: AI can solve complex problems at a human doctorate level. (Demonstrated by the release of OpenAI O3.
3. **Autonomous AI (Level 3)**: AI systems operate independently, managing tasks without human oversight for extended periods, improving business operations.
4. **Innovating AI (Level 4)**: AI can autonomously generate new ideas and improve processes and critical thinking to achieve better outcomes.
5. **Organizational AI (Level 5)**: AI runs entire organizations, handling all functions and collaborating seamlessly to outperform human capabilities across all tasks.

---

## 3. Sequoia Capital - AI Outlook in 2025

In 2024, AI's ecosystem has shifted from potential to tangible, with major players like Google, OpenAI, Anthropic, xAI, and Meta advancing their strategies. Predictions for 2025 include AI search engines becoming essential, driven by improved AI-specific search tools. Also, Big Tech's heavy CapEx investments should stabilize, but the focus will be on delivering results and maximizing ROI. AI's infrastructure is solidifying, and data centers are now key to supporting its growth. For more details, check the [full article](https://arcprize.org/blog/oai-o3-pub-breakthrough).

---

That’s it for this week. 

