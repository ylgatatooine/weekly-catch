# Blog
# Learning about AI, Cloud Computing, Financial Situation



# Financial situation 

This week's economy data, I see that the U.S. economy is demonstrating resilience and strength, as highlighted by several key indicators. Major U.S. financial institutions - the largest banks like JP Morgan, Wells Fargo - reported their quarterly earnings last week, revealing strong revenue and profit figures. These results underscore the robustness of the financial sector and are a positive signal for the broader economy.

Furthermore, the latest Consumer Price Index (CPI) reports—though backward-looking indicators—show that major drivers of inflation are remaining low. This moderation in inflation provides additional stability to the economic landscape, reinforcing optimism about the future.

The U.S. economy stands in contrast to struggling European economies like Germany and the UK, where economic contraction is evident. However, the concerns of US stock market performance come from government debts. High levels of government debt can lead to increased issuance of U.S. Treasury bonds, which may drive up yields on the 10-year bond as investors demand higher returns to compensate for perceived risks. Rising bond yields can increase borrowing costs across the economy and reduce the appeal of equities, often putting downward pressure on the stock market. While some fear that a sudden market adjustment could occur if conditions worsen, there is currently no concrete evidence suggesting such a storm is imminent. The financial markets, for now, remain stable and with rooms to even go higher. 

With President Trump set to be sworn into office tomorrow, I anticipate significant economic changes, particularly in the areas of Trade policies, tariffs, and the value of the U.S. dollar, as well as revival of cryptocurrency. I will also pay attention to DOGE—a program that might even influence perceptions of government debt. The direction of government debt and fiscal policy could take various unpredictable turns, leaving the economy hanging in the balance. It’s a time of potential volatility and change, so I’m staying vigilant and sharp to navigate what’s ahead.


# Blog

# 𝐌𝐞𝐦𝐨𝐫𝐲 𝐢𝐬 𝐀𝐥𝐥 𝐖𝐞 𝐆𝐨𝐭

𝐓𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐞𝐫𝐬' 𝐀𝐜𝐡𝐢𝐥𝐥𝐞𝐬 𝐇𝐞𝐞𝐥

ChatGPT-like transformers are overtaking the tech world but are super expensive to scale. Computational cost, including memory, grows 𝐪𝐮𝐚𝐝𝐫𝐚𝐭𝐢𝐜𝐚𝐥𝐥𝐲 (𝐧*𝐧) with input length. 

It is because of 1. Quadratic Complexity: Attention computation grows exponentially with input size. 2. Memory Bottlenecks: High memory demands limit long-context handling.

Imagine engineers who want to create more prominent models; NVIDIA’s Blackwell feels too tiny before they can have them! (Should I sell my car to buy NVIDIA stock or invest in memory makers instead? Not really.)

𝐌𝐢𝐭𝐢𝐠𝐚𝐭𝐢𝐨𝐧 𝐒𝐭𝐫𝐚𝐭𝐞𝐠𝐢𝐞𝐬

Researchers jumped on the challenge and made much progress. 
1. Sparse Attention Patterns: Reduce computations by focusing only on critical token subsets.
2. Low-Rank Approximations: Simplify attention matrices to save memory.
3. Efficient Attention Variants: Models like Linformer achieve linear complexity \(O(n)\).
4. Hierarchical Models: Process inputs in chunks for scalable context handling.

Here comes Google 𝐓𝐢𝐭𝐚𝐧𝐬: 𝐀 𝐁𝐫𝐞𝐚𝐤𝐭𝐡𝐫𝐨𝐮𝐠𝐡. 

https://lnkd.in/geU-qZ8b

With Titans, Google introduces a 𝐍𝐞𝐮𝐫𝐚𝐥 𝐋𝐨𝐧𝐠-𝐓𝐞𝐫𝐦 𝐌𝐞𝐦𝐨𝐫𝐲 𝐌𝐨𝐝𝐮𝐥𝐞, and new 𝐌𝐞𝐦𝐨𝐫𝐲 𝐌𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭 𝐀𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞 that make AI memory one step closer to what humans get. 

Titans have
1. Short-Term Memory: Captures immediate dependencies.
2. Long-Term Memory: Preserves critical information over long sequences.
3. Persistent Memory: Maintains invariant knowledge.

Titans use
1. Memory as a Gate: Retains or discards information based on surprise-based signals.
2. Memory as a Layer: Integrates memory directly into the architecture.
3. Memory as a Context: Extends context windows by combining short- and long-term memories.

𝐀𝐦𝐚𝐳𝐢𝐧𝐠 𝐫𝐞𝐬𝐮𝐥𝐭𝐬

Google Titans can process context windows 𝐨𝐯𝐞𝐫 2 𝐦𝐢𝐥𝐥𝐢𝐨𝐧 𝐭𝐨𝐤𝐞𝐧𝐬, comparing ChatGPT-4 models with up to 8,912 tokens. Titans outperform in:
- Language Modeling
- Commonsense Reasoning
- Long-context tasks like “needle-in-a-haystack” problems.

Looking around, I also noticed Meta’s innovations, which tackled transformer scaling differently with a focus on sparse patterns :
1. Longformer: Combines local and global attention to handle sequences up to 32,768 tokens.
2. Big Bird: Uses sparse attention with sliding windows for linear complexity.
3. MEGABYTE: Byte-level tokenization and blockwise attention cut memory usage.

Maybe future AI models will cooperate with all those innovations. 

So, I am not rushing to sell my car to buy AI hardware manufacturers' stock. They are too expensive, and I need my car. Is Titans the secret sauce for AI’s future? Who knows? For now, let's marvel at the brilliance of these models—and I hope we all can soon afford the GPUs!

hashtag#AGI hashtag#ArtificialIntelligence hashtag#Singularity hashtag#EthicalAI hashtag#AI hashtag#Technology