# Blog
# Learning about AI, Cloud Computing, Financial Situation



# Financial situation 

This week's economy data, I see that the U.S. economy is demonstrating resilience and strength, as highlighted by several key indicators. Major U.S. financial institutions - the largest banks like JP Morgan, Wells Fargo - reported their quarterly earnings last week, revealing strong revenue and profit figures. These results underscore the robustness of the financial sector and are a positive signal for the broader economy.

Furthermore, the latest Consumer Price Index (CPI) reportsâ€”though backward-looking indicatorsâ€”show that major drivers of inflation are remaining low. This moderation in inflation provides additional stability to the economic landscape, reinforcing optimism about the future.

The U.S. economy stands in contrast to struggling European economies like Germany and the UK, where economic contraction is evident. However, the concerns of US stock market performance come from government debts. High levels of government debt can lead to increased issuance of U.S. Treasury bonds, which may drive up yields on the 10-year bond as investors demand higher returns to compensate for perceived risks. Rising bond yields can increase borrowing costs across the economy and reduce the appeal of equities, often putting downward pressure on the stock market. While some fear that a sudden market adjustment could occur if conditions worsen, there is currently no concrete evidence suggesting such a storm is imminent. The financial markets, for now, remain stable and with rooms to even go higher. 

With President Trump set to be sworn into office tomorrow, I anticipate significant economic changes, particularly in the areas of Trade policies, tariffs, and the value of the U.S. dollar, as well as revival of cryptocurrency. I will also pay attention to DOGEâ€”a program that might even influence perceptions of government debt. The direction of government debt and fiscal policy could take various unpredictable turns, leaving the economy hanging in the balance. Itâ€™s a time of potential volatility and change, so Iâ€™m staying vigilant and sharp to navigate whatâ€™s ahead.


# Blog

# ğŒğğ¦ğ¨ğ«ğ² ğ¢ğ¬ ğ€ğ¥ğ¥ ğ–ğ ğ†ğ¨ğ­

ğ“ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğğ«ğ¬' ğ€ğœğ¡ğ¢ğ¥ğ¥ğğ¬ ğ‡ğğğ¥

ChatGPT-like transformers are overtaking the tech world but are super expensive to scale. Computational cost, including memory, grows ğªğ®ğšğğ«ğšğ­ğ¢ğœğšğ¥ğ¥ğ² (ğ§*ğ§) with input length. 

It is because of 1. Quadratic Complexity: Attention computation grows exponentially with input size. 2. Memory Bottlenecks: High memory demands limit long-context handling.

Imagine engineers who want to create more prominent models; NVIDIAâ€™s Blackwell feels too tiny before they can have them! (Should I sell my car to buy NVIDIA stock or invest in memory makers instead? Not really.)

ğŒğ¢ğ­ğ¢ğ ğšğ­ğ¢ğ¨ğ§ ğ’ğ­ğ«ğšğ­ğğ ğ¢ğğ¬

Researchers jumped on the challenge and made much progress. 
1. Sparse Attention Patterns: Reduce computations by focusing only on critical token subsets.
2. Low-Rank Approximations: Simplify attention matrices to save memory.
3. Efficient Attention Variants: Models like Linformer achieve linear complexity \(O(n)\).
4. Hierarchical Models: Process inputs in chunks for scalable context handling.

Here comes Google ğ“ğ¢ğ­ğšğ§ğ¬: ğ€ ğğ«ğğšğ¤ğ­ğ¡ğ«ğ¨ğ®ğ ğ¡. 

https://lnkd.in/geU-qZ8b

With Titans, Google introduces a ğğğ®ğ«ğšğ¥ ğ‹ğ¨ğ§ğ -ğ“ğğ«ğ¦ ğŒğğ¦ğ¨ğ«ğ² ğŒğ¨ğğ®ğ¥ğ, and new ğŒğğ¦ğ¨ğ«ğ² ğŒğšğ§ğšğ ğğ¦ğğ§ğ­ ğ€ğ«ğœğ¡ğ¢ğ­ğğœğ­ğ®ğ«ğ that make AI memory one step closer to what humans get. 

Titans have
1. Short-Term Memory: Captures immediate dependencies.
2. Long-Term Memory: Preserves critical information over long sequences.
3. Persistent Memory: Maintains invariant knowledge.

Titans use
1. Memory as a Gate: Retains or discards information based on surprise-based signals.
2. Memory as a Layer: Integrates memory directly into the architecture.
3. Memory as a Context: Extends context windows by combining short- and long-term memories.

ğ€ğ¦ğšğ³ğ¢ğ§ğ  ğ«ğğ¬ğ®ğ¥ğ­ğ¬

Google Titans can process context windows ğ¨ğ¯ğğ« 2 ğ¦ğ¢ğ¥ğ¥ğ¢ğ¨ğ§ ğ­ğ¨ğ¤ğğ§ğ¬, comparing ChatGPT-4 models with up to 8,912 tokens. Titans outperform in:
- Language Modeling
- Commonsense Reasoning
- Long-context tasks like â€œneedle-in-a-haystackâ€ problems.

Looking around, I also noticed Metaâ€™s innovations, which tackled transformer scaling differently with a focus on sparse patterns :
1. Longformer: Combines local and global attention to handle sequences up to 32,768 tokens.
2. Big Bird: Uses sparse attention with sliding windows for linear complexity.
3. MEGABYTE: Byte-level tokenization and blockwise attention cut memory usage.

Maybe future AI models will cooperate with all those innovations. 

So, I am not rushing to sell my car to buy AI hardware manufacturers' stock. They are too expensive, and I need my car. Is Titans the secret sauce for AIâ€™s future? Who knows? For now, let's marvel at the brilliance of these modelsâ€”and I hope we all can soon afford the GPUs!

hashtag#AGI hashtag#ArtificialIntelligence hashtag#Singularity hashtag#EthicalAI hashtag#AI hashtag#Technology