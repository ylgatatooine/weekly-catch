# ğ’ğ­ğšğ² ğ‚ğ®ğ«ğ¢ğ¨ğ®ğ¬ â€” ğ“ğ¡ğ ğŸ—ğŸ% ğŒğğ¦ğ¨ğ«ğ² ğ„ğŸğŸğ¢ğœğ¢ğğ§ğœğ² ğğ«ğğšğ¤ğ­ğ¡ğ«ğ¨ğ®ğ ğ¡ ğ¢ğ§ ğ†ğğ§ğ€ğˆ â£â£â£â£

Do you still pay close attention to AI LLM releases? ChatGPT ğŸ’.ğŸ“, Anthropic  ğ‚ğ¥ğšğ®ğğ ğŸ‘.ğŸ•, and ğ‚ğ¥ğšğ®ğğ ğ‚ğ¨ğğ dropped in the last two weeks. Are you jumping on the new products? Letâ€™s face itâ€”except for die-hard fans, who's keeping up with so many changes?â£â£

So, do new LLM releases now feel like the minor, incremental phone upgrades we get every year?

ğğ¨ - They are not the same. Weâ€™re getting new LLMs and their product every few weeks, not once a year.â£â£ More importantly, many AI capabilities have been improved in each release. Take core memory efficiency as an example. With new algorithms ğŒğ‡ğ€ -> ğ†ğğ€ -> ğŒğ‹ğ€, improvement is ğ¨ğ§ğ ğ¨ğ«ğğğ« ğ¨ğŸ ğ¦ğšğ ğ§ğ¢ğ­ğ®ğğ, 90%+,  in a year.â£â£

All LLMs (ğ†ğğ“, ğ‹ğ¥ğšğ¦ğš, ğ†ğğ¦ğ¢ğ§ğ¢) run on Transformer architecture. Attention mechanisms help models focus on relevant information but come at a costâ€”colossal memory overhead. The more tokens, the more storage needed for key-value (KV) pairs. For example, each token in the KV cache can be 4MB. A long query can take hundreds of GB. 

In April 2024, ğ‹ğ¥ğšğ¦ğšğŸ‘ (https://lnkd.in/gnu5nMEg) already improved things with ğ†ğğ€, cutting memory use per token by 8x (https://lnkd.in/g6WhF6_2

The latest breakthrough in late 2024 is ğŒğ‹ğ€ (ğŒğ®ğ¥ğ­ğ¢-ğ‡ğğšğ ğ‹ğšğ­ğğ§ğ­ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§). It applies ğ‹ğšğ­ğğ§ğ­ ğ’ğ©ğšğœğ ğœğ¨ğ¦ğ©ğ«ğğ¬ğ¬ğ¢ğ¨ğ§, a mature Neural Network technique, to the Transformer KV cache, massively optimizing attention and token memory consumption. Deepseek Research paper (https://lnkd.in/g_irva6c) shows that MLA, applied to any transformer-based inference, can ğ«ğğğ®ğœğ ğ¦ğğ¦ğ¨ğ«ğ² ğ®ğ¬ğšğ ğ ğ›ğ² ğŸ—ğŸ% ğ°ğ¢ğ­ğ¡ ğŸ.ğŸ“% ğ©ğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğ¥ğ¨ğ¬ğ¬. Thatâ€™s lovely.â£â£

â£So whatâ€™s next? 

Will MLA become the new standard, and will an even better architecture soon be released? Could I eventually run a smooth LLM on a ğ‘ğšğ¬ğ©ğ›ğğ«ğ«ğ² ğğ¢? Right now, ğ¢ğ­ ğ«ğ®ğ§ğ¬ ... but super ğ¬ğ¥ğ¨ğ°ğ¥ğ². Maybe soon, weâ€™ll have independent ğ€ğˆ agents running on wearablesâ€”likely an ğ€ğ©ğ©ğ¥ğ ğ¢ğ–ğšğ­ğœğ¡ or a ğ†ğšğ«ğ¦ğ¢ğ§ ğ«ğ®ğ§ğ§ğğ«'ğ¬ ğ°ğšğ­ğœğ¡.â£â£

#AI #MachineLearning #MemoryEfficiency #Meta #ChatGPT #TechBreakthroughs #LLM #AIArchitecture

